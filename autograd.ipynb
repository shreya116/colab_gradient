{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autograd.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9D0tb1I5oGUuBC7pcimKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreya116/google_colab/blob/main/autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLTKmvc85uPe"
      },
      "source": [
        "import torch\n",
        "# from torch import nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir5CkGHQnPeO"
      },
      "source": [
        "# class Model(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Model, self).__init__()\n",
        "#         self.linear = nn.Linear(1, 1, bias=False) \n",
        "#     def forward(self, x):\n",
        "#         y_pred = self.linear(x)\n",
        "#         return y_pred\n",
        "# model = Model()"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB9Fu8C7ad7U"
      },
      "source": [
        "x_data = torch.tensor([2104.0, 1600.0, 2400.0])\n",
        "y_data = torch.tensor([399.9, 329.9, 369.0])"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4hUsoqebFiu"
      },
      "source": [
        "w1 = torch.tensor([0.1],requires_grad=True)  #a random guess\n",
        "w2 = torch.tensor([0.1],requires_grad=True)\n",
        "b = torch.tensor([0.0],requires_grad=True)\n",
        "step = 0.00000000000001"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yUC9CqNbT7h"
      },
      "source": [
        "# forward pass\n",
        "def forward(x):\n",
        "  return ((pow(x,2) * w2) + (x * w1) + b)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvHBQT2Lbje4"
      },
      "source": [
        "# mse Loss function\n",
        "loss_fn= F.mse_loss"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvVNVG1HcEGp",
        "outputId": "52b787e4-b99c-4763-f63f-329b6f05a0c3"
      },
      "source": [
        "# Before training\n",
        "print(\"predict (before training)\",  4, forward(2000))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor([400200.], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLlrBDYucHLu",
        "outputId": "1c77ee68-2e36-4ba7-ffa5-d7b784c5af9f"
      },
      "source": [
        "# Training loop \n",
        "for epoch in range(100):\n",
        "  for x_val, y_val in zip(x_data, y_data): \n",
        "    y_pred = forward(x_val)\n",
        "    l = loss_fn(y_pred, y_val)\n",
        "    #print(l)\n",
        "    l.backward() #calculate gradient \n",
        "    with torch.no_grad():\n",
        "      w1-= step * w1.grad\n",
        "      w2-= step * w2.grad\n",
        "      b-= step * b.grad      \n",
        "      print(\"\\tgrad: \", x_val, y_val, w1.grad.item(),w2.grad.item(),b.grad.item())\n",
        "  #updating the gradients to zero after updating\n",
        "  w1.grad.zero_()\n",
        "  w2.grad.zero_()\n",
        "  b.grad.zero_()\n",
        "  print(\"progress:\", epoch, \"w1=\", w1, \"w2=\",w2, \"loss=\", l, \"bias=\",b) "
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tgrad:  tensor(2104.) tensor(399.9000) 1862006784.0 3917662126080.0 884984.1875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) 2359728128.0 4714016014336.0 1196060.0\n",
            "\tgrad:  tensor(2400.) tensor(369.) 2737422080.0 5620481654784.0 1353432.5\n",
            "progress: 0 w1= tensor([0.0999], requires_grad=True) w2= tensor([-0.0425], requires_grad=True) loss= tensor(6.1915e+09, grad_fn=<MseLossBackward>) bias= tensor([-3.4345e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -792892096.0 -1668244963328.0 -376849.84375\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -1005110336.0 -2007794188288.0 -509486.25\n",
            "\tgrad:  tensor(2400.) tensor(369.) -1165015808.0 -2391567237120.0 -576113.5\n",
            "progress: 1 w1= tensor([0.1000], requires_grad=True) w2= tensor([0.0182], requires_grad=True) loss= tensor(1.1098e+09, grad_fn=<MseLossBackward>) bias= tensor([-1.9720e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) 337384480.0 709856985088.0 160353.84375\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) 427410528.0 853898690560.0 216620.125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 496378464.0 1019421786112.0 245356.765625\n",
            "progress: 2 w1= tensor([0.0999], requires_grad=True) w2= tensor([-0.0077], requires_grad=True) loss= tensor(2.0645e+08, grad_fn=<MseLossBackward>) bias= tensor([-2.5944e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -143810976.0 -302578302976.0 -68351.2265625\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -182460160.0 -364416991232.0 -92506.96875\n",
            "\tgrad:  tensor(2400.) tensor(369.) -210931104.0 -432747249664.0 -104369.859375\n",
            "progress: 3 w1= tensor([0.1000], requires_grad=True) w2= tensor([0.0033], requires_grad=True) loss= tensor(35182068., grad_fn=<MseLossBackward>) bias= tensor([-2.3291e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) 61049564.0 128448282624.0 29015.953125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) 77181608.0 154259554304.0 39098.48046875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 90193560.0 185488228352.0 44520.125\n",
            "progress: 4 w1= tensor([0.1000], requires_grad=True) w2= tensor([-0.0014], requires_grad=True) loss= tensor(7348561.5000, grad_fn=<MseLossBackward>) bias= tensor([-2.4418e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -26166214.0 -55053713408.0 -12436.4130859375\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -33356316.0 -66557878272.0 -16930.2265625\n",
            "\tgrad:  tensor(2400.) tensor(369.) -38004980.0 -77714669568.0 -18867.169921875\n",
            "progress: 5 w1= tensor([0.1000], requires_grad=True) w2= tensor([0.0006], requires_grad=True) loss= tensor(937937.3750, grad_fn=<MseLossBackward>) bias= tensor([-2.3935e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) 10964373.0 23069042688.0 5211.2041015625\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) 13703265.0 27451269120.0 6923.01171875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 16573298.0 34339348480.0 8118.85888671875\n",
            "progress: 6 w1= tensor([0.1000], requires_grad=True) w2= tensor([-0.0002], requires_grad=True) loss= tensor(357512.6250, grad_fn=<MseLossBackward>) bias= tensor([-2.4138e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -4843322.0 -10190350336.0 -2301.959228515625\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -6331525.0 -12571474944.0 -3232.0859375\n",
            "\tgrad:  tensor(2400.) tensor(369.) -6662446.0 -13365685248.0 -3369.9697265625\n",
            "progress: 7 w1= tensor([0.1000], requires_grad=True) w2= tensor([0.0001], requires_grad=True) loss= tensor(4752.9766, grad_fn=<MseLossBackward>) bias= tensor([-2.4049e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) 1886526.875 3969252608.0 896.6382446289062\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) 2197935.5 4467506688.0 1091.2686767578125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 3229764.5 6943896064.0 1521.1973876953125\n",
            "progress: 8 w1= tensor([0.1000], requires_grad=True) w2= tensor([-9.7026e-06], requires_grad=True) loss= tensor(46209.6758, grad_fn=<MseLossBackward>) bias= tensor([-2.4084e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -978588.625 -2058950528.0 -465.1086730957031\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -1433333.0 -2786541568.0 -749.3239135742188\n",
            "\tgrad:  tensor(2400.) tensor(369.) -981671.0 -1702552704.0 -561.1314086914062\n",
            "progress: 9 w1= tensor([0.1000], requires_grad=True) w2= tensor([5.5778e-05], requires_grad=True) loss= tensor(8854.1045, grad_fn=<MseLossBackward>) bias= tensor([-2.4066e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) 241184.234375 507451616.0 114.63128662109375\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) 112616.171875 301742720.0 34.2762451171875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 811274.0 1978521472.0 325.3836669921875\n",
            "progress: 10 w1= tensor([0.1000], requires_grad=True) w2= tensor([2.7901e-05], requires_grad=True) loss= tensor(21185.8828, grad_fn=<MseLossBackward>) bias= tensor([-2.4071e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(399.9000) -278112.75 -585149184.0 -132.182861328125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -545544.6875 -1013040256.0 -299.32781982421875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 47959.0 411368704.0 -52.03460693359375\n",
            "progress: 11 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.9769e-05], requires_grad=True) loss= tensor(15288.4834, grad_fn=<MseLossBackward>) bias= tensor([-2.4066e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -57031.3203125 -119993896.0 -27.10614013671875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -265344.40625 -453294816.0 -157.30181884765625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 372926.71875 1078555776.0 108.64447021484375\n",
            "progress: 12 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.4716e-05], requires_grad=True) loss= tensor(17681.8574, grad_fn=<MseLossBackward>) bias= tensor([-2.4065e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -151152.703125 -318025280.0 -71.84063720703125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -384634.625 -691596416.0 -217.766845703125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 234577.6875 794513152.0 40.23828125\n",
            "progress: 13 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6867e-05], requires_grad=True) loss= tensor(16641.6621, grad_fn=<MseLossBackward>) bias= tensor([-2.4063e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -111082.1875 -233716912.0 -52.79571533203125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -333848.875 -590143616.0 -192.02490234375\n",
            "\tgrad:  tensor(2400.) tensor(369.) 293477.1875 915438976.0 69.3609619140625\n",
            "progress: 14 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.5952e-05], requires_grad=True) loss= tensor(17080.6426, grad_fn=<MseLossBackward>) bias= tensor([-2.4061e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -128141.484375 -269609696.0 -60.90374755859375\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -355469.90625 -633335168.0 -202.9840087890625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 268401.78125 863956864.0 56.9625244140625\n",
            "progress: 15 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6341e-05], requires_grad=True) loss= tensor(16893.0508, grad_fn=<MseLossBackward>) bias= tensor([-2.4059e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -120878.6796875 -254328736.0 -57.45184326171875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -346265.0 -614946816.0 -198.31829833984375\n",
            "\tgrad:  tensor(2400.) tensor(369.) 279077.375 885874944.0 62.24102783203125\n",
            "progress: 16 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6175e-05], requires_grad=True) loss= tensor(16972.7910, grad_fn=<MseLossBackward>) bias= tensor([-2.4057e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123970.984375 -260834944.0 -58.92156982421875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -350184.1875 -622776064.0 -200.3048095703125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 274532.4375 876543744.0 59.9937744140625\n",
            "progress: 17 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6246e-05], requires_grad=True) loss= tensor(16938.8379, grad_fn=<MseLossBackward>) bias= tensor([-2.4055e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -122654.4453125 -258064944.0 -58.29583740234375\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -348515.6875 -619442944.0 -199.4591064453125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 276467.25 880516096.0 60.950439453125\n",
            "progress: 18 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6216e-05], requires_grad=True) loss= tensor(16953.2832, grad_fn=<MseLossBackward>) bias= tensor([-2.4053e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123214.7265625 -259243792.0 -58.5621337890625\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349225.75 -620861440.0 -199.81903076171875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275643.75 878825472.0 60.54327392578125\n",
            "progress: 19 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6229e-05], requires_grad=True) loss= tensor(16947.1328, grad_fn=<MseLossBackward>) bias= tensor([-2.4051e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -122976.2578125 -258742048.0 -58.44879150390625\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -348923.53125 -620257664.0 -199.66583251953125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275994.03125 879544576.0 60.71649169921875\n",
            "progress: 20 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6223e-05], requires_grad=True) loss= tensor(16949.7383, grad_fn=<MseLossBackward>) bias= tensor([-2.4049e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123077.8359375 -258955760.0 -58.4970703125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349052.25 -620514816.0 -199.7310791015625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275844.8125 879238144.0 60.6427001953125\n",
            "progress: 21 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6226e-05], requires_grad=True) loss= tensor(16948.6270, grad_fn=<MseLossBackward>) bias= tensor([-2.4047e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123034.5625 -258864720.0 -58.47650146484375\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -348997.375 -620405184.0 -199.7032470703125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275908.5 879368896.0 60.6741943359375\n",
            "progress: 22 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16949.1035, grad_fn=<MseLossBackward>) bias= tensor([-2.4045e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123053.0546875 -258903616.0 -58.48529052734375\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349020.84375 -620452096.0 -199.71514892578125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275881.53125 879313536.0 60.66082763671875\n",
            "progress: 23 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9121, grad_fn=<MseLossBackward>) bias= tensor([-2.4043e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123045.0859375 -258886864.0 -58.48150634765625\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349010.71875 -620431872.0 -199.71002197265625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275893.09375 879337216.0 60.66656494140625\n",
            "progress: 24 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9922, grad_fn=<MseLossBackward>) bias= tensor([-2.4041e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123048.4296875 -258893888.0 -58.48309326171875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349015.03125 -620440448.0 -199.71221923828125\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275887.90625 879326592.0 60.66400146484375\n",
            "progress: 25 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9434, grad_fn=<MseLossBackward>) bias= tensor([-2.4039e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.140625 -258891184.0 -58.48248291015625\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.25 -620436992.0 -199.7113037109375\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275890.25 879331456.0 60.6651611328125\n",
            "progress: 26 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9766, grad_fn=<MseLossBackward>) bias= tensor([-2.4037e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 27 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4035e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 28 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4033e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 29 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4031e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 30 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4029e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 31 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4027e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 32 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4025e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 33 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4023e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 34 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4021e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 35 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4020e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 36 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4018e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 37 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4016e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 38 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4014e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 39 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4012e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 40 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4010e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 41 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4008e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 42 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4006e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 43 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4004e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 44 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4002e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 45 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.4000e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 46 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3998e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 47 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3996e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 48 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3994e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 49 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3992e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 50 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3990e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 51 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3988e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 52 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3986e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 53 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3984e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 54 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3982e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 55 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3980e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 56 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3978e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 57 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3976e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 58 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3974e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 59 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3972e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 60 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3970e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 61 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3968e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 62 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3966e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 63 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3964e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 64 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3962e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 65 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3960e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 66 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3958e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 67 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3956e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 68 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3954e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 69 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3952e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 70 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3950e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 71 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3948e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 72 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3946e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 73 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3945e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 74 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3943e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 75 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3941e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 76 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3939e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 77 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3937e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 78 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3935e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 79 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3933e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 80 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3931e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 81 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3929e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 82 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3927e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 83 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3925e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 84 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3923e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 85 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3921e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 86 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3919e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 87 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3917e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 88 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3915e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 89 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3913e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 90 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3911e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 91 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3909e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 92 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3907e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 93 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3905e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 94 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3903e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 95 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3901e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 96 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3899e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 97 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3897e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.3984375 -258891728.0 -58.48260498046875\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.71875 -620437824.0 -199.7115478515625\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.53125 879329856.0 60.664794921875\n",
            "progress: 98 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3895e-08], requires_grad=True)\n",
            "\tgrad:  tensor(2104.) tensor(399.9000) -123047.65625 -258892272.0 -58.48272705078125\n",
            "\tgrad:  tensor(1600.) tensor(329.9000) -349013.96875 -620438336.0 -199.711669921875\n",
            "\tgrad:  tensor(2400.) tensor(369.) 275889.28125 879329344.0 60.6646728515625\n",
            "progress: 99 w1= tensor([0.1000], requires_grad=True) w2= tensor([3.6225e-05], requires_grad=True) loss= tensor(16948.9609, grad_fn=<MseLossBackward>) bias= tensor([-2.3893e-08], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z5Q_AltchRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cdb0079-6e8e-49b5-95f6-7e698bdbc650"
      },
      "source": [
        "# After training\n",
        "print(\"predict (after training)\",  \"4 hours\", forward(2000))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (after training) 4 hours tensor([344.8024], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}